{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to teach Artificial Intellegence to play Games\n",
    "## Computer learning how to play a game of Frozen Lake using Reinforcement Learning\n",
    "\n",
    "\n",
    "![image.png](frozen_lake.png)\n",
    "\n",
    "The cool part here is that we'll not explain the game or the rules to computer. we'll provide it with needed intellegence and let it play the game 100,000 times (in a few seconds) and it'll learn how to to play the game and improve the playing skills. at the end we'll see performance and watch it play three games live!\n",
    "\n",
    "\n",
    "## Frozen lake - Game description\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "Source: https://gym.openai.com/envs/FrozenLake8x8-v0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import key liberaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "# the liberary where the game sits\n",
    "import gym\n",
    "\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake8x8-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate Q-table \n",
    "action_space_size = env.action_space.n  # number of possible movements at each position \"state\"\n",
    "state_space_size = env.observation_space.n  # number of possible locations \"states\" in game\n",
    "\n",
    "# initialising Q-Table\n",
    "q_table = np.zeros((state_space_size , action_space_size))\n",
    "\n",
    "# Initialising Q-learning parameters\n",
    "num_episodes = 100_000   # number of episodes played in training\n",
    "max_steps_per_episod = 500   # max number of steps per game (to avoid having a game run for a very long time)\n",
    "\n",
    "# setting the parameters for exploration vs exploitation\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The equation used to fill learning q_table\n",
    "\n",
    "\n",
    "\\begin{eqnarray*}q^{new}\\left(s,a\\right)=\\left(1-\\alpha\\right)\\underset{\\text{old value}}{\\underbrace{q\\left(s,a\\right)}}+\\alpha\\overset{\\text{learned value}}{\\overbrace{\\left(R_{t+1}+\\gamma\\max_{a^{\\prime}}q\\left(s^{\\prime},a^{\\prime}\\right)\\right)}}\\end{eqnarray*}\n",
    "\n",
    "\n",
    "                                                            \n",
    "$\\alpha$:    **Learning rate**  \n",
    "Is how quickly the agent abandons the previous Q-value in the Q-table for the new Q-value\n",
    "- 1: ignore past learnings\n",
    "- 0: ignore new learning\n",
    "\n",
    "$R_{t+1}$: **Reward gained after doing the action**\n",
    "\n",
    "$\\gamma$:    **Discount rate**  \n",
    "number less than 1 to represent uncertainty (around getting this future reward) and makes the algorithm converge\n",
    "\n",
    "\n",
    "an example from this game will look like this:  \n",
    "\n",
    "\\begin{eqnarray*}\\max_{a^{\\prime}}q\\left(s^{\\prime},a^{\\prime}\\right)&=&\\max\\left(q\\left(\\text{next_state,left}\\right),q\\left(\\text{next_state,right}\\right),q\\left(\\text{next_state,up}\\right),q\\left(\\text{next_state,down}\\right)\\right)\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting ket parameters/constants for Q_learning equation\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = []\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes): \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episod):\n",
    "        exploration_rate_threshold = random.uniform(0,1) # pick a random number from a uniform distribution from 0 - 1\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "        learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward \n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "    \n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    # Append current episode rewards\n",
    "    rewards_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per ten thousands episodes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no of episodes played</th>\n",
       "      <th>average success rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>3.98%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000</td>\n",
       "      <td>25.55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30000</td>\n",
       "      <td>51.74%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40000</td>\n",
       "      <td>68.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>74.57%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60000</td>\n",
       "      <td>78.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70000</td>\n",
       "      <td>78.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80000</td>\n",
       "      <td>79.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>90000</td>\n",
       "      <td>80.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100000</td>\n",
       "      <td>79.92%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no of episodes played average success rate\n",
       "0                  10000                3.98%\n",
       "1                  20000               25.55%\n",
       "2                  30000               51.74%\n",
       "3                  40000               68.03%\n",
       "4                  50000               74.57%\n",
       "5                  60000               78.08%\n",
       "6                  70000               78.67%\n",
       "7                  80000               79.67%\n",
       "8                  90000               80.72%\n",
       "9                 100000               79.92%"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and print the average reward per ten thousands episodes\n",
    "rewards_per_ten_thosands_episodes = np.split(np.array(rewards_all_episodes),num_episodes/10_000)\n",
    "count = 10_000\n",
    "\n",
    "count_list = []\n",
    "average_success = []\n",
    "\n",
    "for r in rewards_per_ten_thosands_episodes:\n",
    "    count_list.append(count)\n",
    "    average_success.append(sum(r/10000))\n",
    "    count += 10000\n",
    "    \n",
    "# print performance results\n",
    "performance = pd.DataFrame({'no of episodes played':count_list , 'average success rate': average_success })\n",
    "performance['average success rate'] = performance['average success rate'].map(\"{:.2%}\".format)\n",
    "print(\"Average reward per ten thousands episodes\")\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print resulting Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States are named in the following order \n",
      "0 1 2 3 4 5 6 7\n",
      "8 9 10 11 12 13 14 15\n",
      "...etc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0- Left</th>\n",
       "      <th>1- Down</th>\n",
       "      <th>2- Right</th>\n",
       "      <th>3- Up</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>States</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.388429</td>\n",
       "      <td>0.389772</td>\n",
       "      <td>0.385271</td>\n",
       "      <td>0.413135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.398320</td>\n",
       "      <td>0.393933</td>\n",
       "      <td>0.423904</td>\n",
       "      <td>0.399463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.411083</td>\n",
       "      <td>0.415226</td>\n",
       "      <td>0.450368</td>\n",
       "      <td>0.415257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.429145</td>\n",
       "      <td>0.436226</td>\n",
       "      <td>0.469922</td>\n",
       "      <td>0.433853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.454277</td>\n",
       "      <td>0.453091</td>\n",
       "      <td>0.480124</td>\n",
       "      <td>0.450888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.055281</td>\n",
       "      <td>0.176328</td>\n",
       "      <td>0.089101</td>\n",
       "      <td>0.005191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.274043</td>\n",
       "      <td>0.263816</td>\n",
       "      <td>0.522880</td>\n",
       "      <td>0.299619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.309052</td>\n",
       "      <td>0.767862</td>\n",
       "      <td>0.457207</td>\n",
       "      <td>0.525562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0- Left   1- Down  2- Right     3- Up\n",
       "States                                        \n",
       "0       0.388429  0.389772  0.385271  0.413135\n",
       "1       0.398320  0.393933  0.423904  0.399463\n",
       "2       0.411083  0.415226  0.450368  0.415257\n",
       "3       0.429145  0.436226  0.469922  0.433853\n",
       "4       0.454277  0.453091  0.480124  0.450888\n",
       "...          ...       ...       ...       ...\n",
       "59      0.000000  0.000000  0.000000  0.000000\n",
       "60      0.055281  0.176328  0.089101  0.005191\n",
       "61      0.274043  0.263816  0.522880  0.299619\n",
       "62      0.309052  0.767862  0.457207  0.525562\n",
       "63      0.000000  0.000000  0.000000  0.000000\n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table = pd.DataFrame(q_table)\n",
    "Q_table.index.rename('States', inplace=True)\n",
    "Q_table.rename(columns={0:'0- Left' , 1:'1- Down' , 2:'2- Right' , 3:'3- Up'}, inplace=True )\n",
    "print('States are named in the following order \\n0 1 2 3 4 5 6 7\\n8 9 10 11 12 13 14 15\\n...etc')\n",
    "Q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let watch the computer playing 3 episodes live and show us the skills gained after 100,000 times playing the game ! ... ENJOY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n",
      "***You win!***\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episod):        \n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"***You win!***\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"***You fell through a hole!***\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To avoid installing the liberary and other supporting software and fix potential bugs, you can run the code on Google Collab*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful resources:\n",
    "- https://deeplizard.com/learn/video/QK_PP_2KgGE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
